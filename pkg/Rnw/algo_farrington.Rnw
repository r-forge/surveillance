
%anscombe


\name{anscombe.residuals}
\alias{anscombe.residuals}
\encoding{latin1}

\title{Compute Anscombe residuals}
\description{
  The residuals of \code{m} are transformed to form Anscombe residuals.
  which makes them approximately standard normal distributed.
}
\usage{
anscombe.residuals(m, phi)
}
\arguments{
  \item{m}{\code{m} is a glm object of the fit }
  \item{phi}{\code{phi} is the current estimated over-dispersion}
}
\value{Standardized Anscombe residuals of \code{m}}
\references{McCullagh & Nelder, Generalized Linear Models, 1989}
\keyword{regression}

<<echo=F>>=
anscombe.residuals <- function(m,phi) {
  y <- m$y
  mu <- fitted.values(m)
  
  #Compute raw Anscombe residuals
  a <- 3/2*(y^(2/3) * mu^(-1/6) - mu^(1/2))
  #Compute standardized residuals
  a <- a/sqrt(phi * (1-hatvalues(m)))
  return(a)
}
@ 


%algo.farrington.assign.weights


\name{algo.farrington.assign.weights}
\alias{algo.farrington.assign.weights}
\encoding{latin1}

\title{Assign weights to base counts}
\description{
  Weights are assigned according to the Anscombe residuals
}
\usage{
algo.farrington.assign.weights(s)
}
\arguments{
  \item{s}{Vector of standardized Anscombe residuals}
}
\value{Weights according to the residuals}
\seealso{See Also as \code{\link{anscombe.residuals}}}
\keyword{regression}

<<echo=F>>=
algo.farrington.assign.weights <- function(s) {
  #s_i^(-2) for s_i<1 and 1 otherwise
  gamma <- length(s)/(sum(  (s^(-2))^(s>1) ))
  omega <- numeric(length(s)) 
  omega[s>1] <- gamma*(s[s>1]^(-2))
  omega[s<=1] <- gamma
  return(omega)
}
@ 

\name{algo.farrington.fitGLM}
\alias{algo.farrington.fitGLM}
\alias{algo.farrington.fitGLM.fast}
\alias{algo.farrington.fitGLM.populationOffset}
\encoding{latin1}

\title{Fit the Poisson GLM of the Farrington procedure for a single
  time point}
\description{
  The function fits a Poisson regression model (GLM) with mean predictor
  \deqn{\log \mu_t = \alpha + \beta w_t}{
        log mu_t = alpha + beta * w}
  as specified by the Farrington procedure. That way we are able to
  predict the value \eqn{c_0}{c0}. If 
  requested Anscombe residuals are computed based on an initial fit
  and a 2nd fit is made using weights, where base counts suspected to
  be caused by earlier outbreaks are downweighted.
}
\usage{
  algo.farrington.fitGLM(response, wtime, timeTrend = TRUE, 
                         reweight = TRUE, ...)
  algo.farrington.fitGLM.fast(response, wtime, timeTrend = TRUE, 
                         reweight = TRUE, ...)
  algo.farrington.fitGLM.populationOffset(response, wtime, population,
                         timeTrend=TRUE,reweight=TRUE, ...) 
}
\arguments{
\item{response}{The vector of observed base counts}
\item{wtime}{Vector of week numbers corresponding to \code{response}}
\item{timeTrend}{Boolean whether to fit the \eqn{\beta t}{beta*t} or not}
\item{reweight}{Fit twice -- 2nd time with Anscombe residuals}
\item{population}{Population size. Possibly used as offset, i.e. in
    \code{algo.farrington.fitGLM.populationOffset} the value 
    \code{log(population)} is used as offset in the linear
    predictor of the GLM:   
    \deqn{\log \mu_t = \log(\texttt{population}) + \alpha + \beta w_t}{
      log mu_t = log(population) alpha + beta * w}
    This provides a way to adjust the Farrington procedure to the case
    of greatly varying populations. Note: This is an experimental implementation with methodology not covered by the original paper.
  }
\item{\dots}{Used to catch additional arguments, currently not used.}
}
%
\details{Compute weights from an initial fit and rescale using
  Anscombe based residuals as described in the
  \code{\link{anscombe.residuals}} function.

  Note that \code{algo.farrington.fitGLM} uses the \code{glm} routine
  for fitting. A faster alternative is provided by
  \code{algo.farrington.fitGLM.fast} which uses the \code{glm.fit}
  function directly (thanks to Mikko Virtanen). This saves
  computational overhead and increases speed for 500 monitored time
  points by a factor of approximately two. However, some of the
  routine \code{glm} functions might not work on the output of this
  function. Which function is used for \code{algo.farrington} can be
  controlled by the \code{control$fitFun} argument.
}
%
\value{An object of class GLM with additional fields \code{wtime},
  \code{response} and \code{phi}. If the \code{glm} returns without
  convergence \code{NULL} is returned.}
%
\seealso{\code{\link{anscombe.residuals}},\code{\link{algo.farrington}}}
\keyword{regression}

<<echo=F>>=
algo.farrington.fitGLM <- function(response,wtime,timeTrend=TRUE,reweight=TRUE,...) {
  #Model formula depends on whether to include a time trend or not.
  theModel <- as.formula(ifelse(timeTrend, "response~1+wtime","response~1"))

  #Fit it -- this is slow. An improvement would be to use glm.fit here.
  model <- glm(theModel, family = quasipoisson(link="log"))
    
 #Check convergence - if no convergence we return empty handed.
  if (!model$converged) {
    #Try without time dependence
    if (timeTrend) {
     model <- glm(response ~ 1, family = quasipoisson(link="log"))
     cat("Warning: No convergence with timeTrend -- trying without.\n")
    } 

    if (!model$converged) {
      cat("Warning: No convergence in this case.\n")
      print(cbind(response,wtime))
      return(NULL)
    }
  }

  #Overdispersion parameter phi
  phi <- max(summary(model)$dispersion,1)
  
  #In case reweighting using Anscome residuals is requested
  if (reweight) {
    s <- anscombe.residuals(model,phi)
    omega <- algo.farrington.assign.weights(s)
    model <- glm(theModel,family=quasipoisson(link="log"),weights=omega)
    #Here, the overdispersion often becomes small, so we use the max
    #to ensure we don't operate with quantities less than 1.
    phi <- max(summary(model)$dispersion,1)
  } # end of refit.
  

  #Add wtime, response and phi to the model
  model$phi <- phi
  model$wtime <- wtime
  model$response <- response
  #Done
  return(model)
}

######################################################################
# The algo.farrington.fitGLM function in a version using glm.fit 
# which is faster than the call using "glm. 
# This saves lots of overhead and increases speed.
#
# Author: Mikko Virtanen (@thl.fi) with minor modifications by Michael Hoehle
# Date:   9 June 2010 
#
# Note: Not all glm results may work on the output. But for the
# necessary ones for the algo.farrington procedure work.
######################################################################

algo.farrington.fitGLM.fast <- function(response,wtime,timeTrend=TRUE,reweight=TRUE, ...) {
  #Create design matrix and formula needed for the terms object
  #Results depends on whether to include a time trend or not.
  if (timeTrend) {
    design<-cbind(intercept=1,wtime=wtime) 
    Formula<-response~wtime 
  } else {
    design<-matrix(1,nrow=length(wtime),dimnames=list(NULL,c("intercept")))
    Formula<-response~1
  }
  
  #Fit it using glm.fit which is faster than calling "glm"
  model <- glm.fit(design,response, family = quasipoisson(link = "log"))
      
   #Check convergence - if no convergence we return empty handed.
   if (!model$converged) {
      #Try without time dependence
     if (timeTrend) {
       model <- glm.fit(design[,1,drop=FALSE],response, family = quasipoisson(link = "log"))
       Formula<-response~1
       cat("Warning: No convergence with timeTrend -- trying without.\n")
     } 
   }

   #Fix class of output to glm/lm object in order for anscombe.residuals to work
   #Note though: not all glm methods may work for the result
   class(model) <- c("glm","lm")

   #Overdispersion parameter phi
   phi <- max(summary.glm(model)$dispersion,1)
    
   #In case reweighting using Anscome residuals is requested
   if (reweight) {
     s <- anscombe.residuals(model,phi)
     omega <- algo.farrington.assign.weights(s)
     model <- glm.fit(design,response, family = quasipoisson(link = "log"), weights = omega)
     #Here, the overdispersion often becomes small, so we use the max
     #to ensure we don't operate with quantities less than 1.
     phi <- max(summary.glm(model)$dispersion,1)
   } # end of refit.
    
   model$phi <- phi
   model$wtime <- wtime
   model$response <- response
   model$terms<-terms(Formula)
   # cheating a bit, all methods for glm may not work
   class(model)<-c("algo.farrington.glm","glm") 
   #Done
  return(model)
}

######################################################################
# Experimental function to include a population offset in the 
# farrington procedure based on algo.farrington.fitGLM
# Alternative: include populationOffset argument in the two other
# fit functions, but I suspect use of this is not so common
#
# Parameters:
#  takes an additional "population" parameter
######################################################################

algo.farrington.fitGLM.populationOffset <- function(response,wtime,population,timeTrend=TRUE,reweight=TRUE,...) {
  #Model formula depends on whether to include a time trend or not.
  theModel <- as.formula(ifelse(timeTrend, "response~offset(log(population)) + 1 + wtime","response~offset(log(population)) + 1"))

  #Fit it -- this is slow. An improvement would be to use glm.fit here.
  model <- glm(theModel, family = quasipoisson(link="log"))
    
 #Check convergence - if no convergence we return empty handed.
  if (!model$converged) {
    #Try without time dependence
    if (timeTrend) {
     model <- glm(response ~ 1, family = quasipoisson(link="log"))
     cat("Warning: No convergence with timeTrend -- trying without.\n")
    } 

    if (!model$converged) {
      cat("Warning: No convergence in this case.\n")
      print(cbind(response,wtime))
      return(NULL)
    }
  }

  #Overdispersion parameter phi
  phi <- max(summary(model)$dispersion,1)
  
  #In case reweighting using Anscome residuals is requested
  if (reweight) {
    s <- anscombe.residuals(model,phi)
    omega <- algo.farrington.assign.weights(s)
    model <- glm(theModel,family=quasipoisson(link="log"),weights=omega)
    #Here, the overdispersion often becomes small, so we use the max
    #to ensure we don't operate with quantities less than 1.
    phi <- max(summary(model)$dispersion,1)
  } # end of refit.
  

  #Add wtime, response and phi to the model
  model$phi <- phi
  model$wtime <- wtime
  model$response <- response
  model$population <- population
  #Done
  return(model)
}


@ 

\name{algo.farrington.threshold}
\alias{algo.farrington.threshold}
\encoding{latin1}

\title{Compute prediction interval for a new observation}
\description{
Depending on the current transformation \eqn{h(y)= \{y, \sqrt{y}, y^{2/3}\}},

  \deqn{V(h(y_0)-h(\mu_0))=V(h(y_0))+V(h(\mu_0))}

  is used to compute a prediction interval. The prediction variance
  consists of a component due to the variance of having a single
  observation and a prediction variance.  }

\usage{
algo.farrington.threshold(pred,phi,alpha=0.01,skewness.transform="none",y)
}
\arguments{
\item{pred}{A GLM prediction object}
\item{phi}{Current overdispersion parameter (superflous?)}
\item{alpha}{Quantile level in Gaussian based CI, i.e. an \eqn{(1-\alpha)\cdot 100\%}
    confidence interval is computed. }
\item{skewness.transform}{Skewness correction, i.e. one of
    \code{"none"}, \code{"1/2"}, or \code{"2/3"}.}
\item{y}{Observed number}
}
\value{
  %
  Vector of length four with lower and upper bounds of an
  \eqn{(1-\alpha)\cdot 100\%} confidence interval (first two
  arguments) and corresponding quantile of observation \code{y}
  together with the median of the predictive distribution.
  %
}
\keyword{regression}

<<echo=F>>=

algo.farrington.threshold <- function(pred,phi,alpha=0.01,skewness.transform="none",y) {
  #Fetch mu0 and var(mu0) from the prediction object
  mu0 <- pred$fit
  tau <- phi + (pred$se.fit^2)/mu0
  #Standard deviation of prediction, i.e. sqrt(var(h(Y_0)-h(\mu_0))) 
  switch(skewness.transform,
         "none" = { se <- sqrt(mu0*tau); exponent <- 1},
         "1/2" = { se <- sqrt(1/4*tau); exponent <- 1/2},
         "2/3"  = { se <- sqrt(4/9*mu0^(1/3)*tau); exponent <- 2/3},
         { stop("No proper exponent in algo.farrington.threshold.")})

  #Note that lu can contain NA's if e.g. (-1.47)^(3/2)
  lu <- sort((mu0^exponent + c(-1,1)*qnorm(1-alpha/2)*se)^(1/exponent),na.last=FALSE)

  #Ensure that lower bound is non-negative
  lu[1] <- max(0,lu[1],na.rm=TRUE)

  #Compute quantiles of the predictive distribution based on the
  #normal approximation on the transformed scale
  q <- pnorm( y^(1/exponent) , mean=mu0^exponent, sd=se)
  m <- qnorm(0.5, mean=mu0^exponent, sd=se)^(1/exponent)
  
  #Return lower and upper bounds
  return(c(lu,q=q,m=m))
}
@ 

\name{refvalIdxByDate}
\alias{refvalIdxByDate}
\encoding{latin1}

\title{Compute indices of reference value using Date class}
\description{ 

  The reference values are formed base on computatations 
  of \code{seq} for Date class arguments.  

}
\usage{
refvalIdxByDate(t0, b, w, epochStr, epochs)
}
\arguments{
\item{t0}{A Date object describing the time point}
\item{b}{Number of years to go back in time}
\item{w}{Half width of window to include reference values for}
\item{epochStr}{One of \code{"1 month"}, \code{"1 week"} or \code{"1 day"}}

\item{epochs}{Vector containing the epoch value of the sts/disProg object}
}
\details{ 

  Using the Date class the reference values are formed as follows:
  Starting from \code{t0} go i, i= 1,...,\code{b} years back in time.
  For each year, go \code{w} epochs back and include from here to
  \code{w} epochs after \code{t0}.

  In case of weeks we always go back to the closest monday of this
  date. In case of months we also go back in time to closest 1st of
  month. 
}

\value{
  a vector of indices in epochs which match
}
\keyword{regression}

<<echo=FALSE>>=
######################################################################
# Compute indices of reference value using Date class
#
# Params:
#  t0 - Date object describing the time point
#  b  - Number of years to go back in time
#  w  - Half width of window to include reference values for
#  epochStr - "1 month", "1 week" or "1 day"
#  epochs - Vector containing the epoch value of the sts/disProg object
#
# Details:
#  Using the Date class the reference values are formed as follows:
#   Starting from d0 go i, i in 1,...,b years back in time.
#   
# Returns:
#  a vector of indices in epochs which match
######################################################################
refvalIdxByDate <- function(t0, b, w, epochStr, epochs) {
  refDays <- NULL
  refPoints <- seq( t0, length=b+1, by="-1 year")[-1]
  for (j in 1:length(refPoints)) {
    refPointWindow <- c(rev(seq(refPoints[j], length=w+1, by=paste("-",epochStr,sep=""))),
                        seq(refPoints[j], length=w+1, by=epochStr)[-1])
    refDays <- append(refDays,refPointWindow)
  }
  if (epochStr == "1 week") {
    #By convention: always go back to the closest monday 
    #(but always back, never ahead!)
    refDays <- refDays -  (as.numeric(format(refDays, "%w")) - 1)
  }
  if (epochStr == "1 month") {
    #By convention: go back in time to  closest 1st of month
    refDays <- refDays -  (as.numeric(format(refDays, "%d")) - 1)
  }
  #Find the index of these reference values
  wtime <- match(as.numeric(refDays), epochs)
  return(wtime)
} 

@ 

\name{algo.farrington}
\alias{algo.farrington}
\encoding{latin1}

\title{Surveillance for a count data time series using the Farrington method.}
\description{
%
  The function takes \code{range} values of the surveillance time
  series \code{disProgObj} and for each time point uses a GLM to
  predict the number of counts according to the procedure by
  Farrington et al. (1996). This is then compared to the observed
  number of counts. If the observation is above a specific quantile of
  the prediction interval, then an alarm is raised.
%
}
\usage{
  algo.farrington(disProgObj, control=list(range=NULL, b=3, w=3,
  reweight=TRUE,verbose=FALSE,alpha=0.01,trend=TRUE,limit54=c(5,4),
  powertrans="2/3",
  fitFun=c("algo.farrington.fitGLM.fast","algo.farrington.fitGLM","algo.farrington.fitGLM.populationOffset")))
}
\arguments{
\item{disProgObj}{object of class disProgObj (including the \code{observed} and the \code{state} time series.)}
\item{control}{Control object
    \describe{
    \item{\code{range}}{Specifies the index of all timepoints which
        should be tested. If range is \code{NULL} the maximum number
        of possible weeks is used.}
    \item{\code{b}}{how many years back in time to include when
        forming the base counts.}
    \item{\code{w}}{windows size, i.e. number of weeks to include
        before and after the current week}
    \item{\code{reweight}}{Boolean specifying whether to perform reweight step}
    \item{\code{trend}}{If \code{true} a trend is included and kept in
        case the conditions in the Farrington et. al. paper are met
        (see the results). If \code{false} then NO trend is fit.}
    \item{\code{verbose}}{show extra debugging information}
    \item{\code{plot}}{shows the final GLM model fit graphically (use
        History|Recording to see all pictures)}
    \item{\code{powertrans}}{Power transformation to apply to the
        data. Use either "2/3" for skewness correction (Default),
        "1/2" for variance stabilizing transformation or "none" for no
        transformation}
    \item{\code{alpha}}{An approximate (two-sided) \eqn{(1-\alpha)\%}\
        prediction interval is calculated}
    \item{\code{limit54}}{To avoid alarms in cases where the time series only
        has about 0-2 cases the algorithm uses the following heuristic
        criterion (see Section 3.8 of the Farrington paper) to protect
        against low counts: no alarm is sounded if fewer than
        \eqn{cases=5} reports were received in the past \eqn{period=4}
        weeks. \code{limit54=c(cases,period)} is a vector allowing the
        user to change these numbers. Note: As of version 0.9-7 The
        term "last" period of weeks includes the current week -
        otherwise no alarm is sounded for horrible large numbers if
        the four weeks before that are too low.}
    \item{\code{fitFun}}{String containing the name of the fit
        function to be used for fitting the GLM. The options are
        \code{algo.farrington.fitGLM.fast} (default) and
        \code{algo.farrington.fitGLM} or 
        \code{algo.farrington.fitGLM.populationOffset}. See details of
        \code{\link{algo.farrington.fitGLM}} for more information.}
      % 
      }
    }
}
\details{
  The following steps are perfomed according to the Farrington
  et al. (1996) paper.
\enumerate{
\item fit of the initial model and initial estimation of mean and
  overdispersion.
\item calculation of the weights omega (correction for past outbreaks)
\item refitting of the model
\item revised estimation of overdispersion
\item rescaled model
\item omission of the trend, if it is not significant
\item repetition of the whole procedure
\item calculation of the threshold value
\item computation of exceedance score
}
}
\value{
An object of class \code{SurvRes}.
}

\examples{
#Read Salmonella Agona data
data("salmonella.agona")

#Do surveillance for the last 100 weeks.
n <- length(salmonella.agona$observed)
#Set control parameters.
control <- list(b=4,w=3,range=(n-100):n,reweight=TRUE, verbose=FALSE,alpha=0.01)
res <- algo.farrington(salmonella.agona,control=control)
#Plot the result.
plot(res,disease="Salmonella Agona",method="Farrington")

\dontrun{
#Generate random data and convert into sts object
set.seed(123)
x <- matrix(rpois(1000,lambda=1),ncol=1)
sts <- new("sts", observed=x, epoch=1:nrow(x), state=x*0, freq=52)

#Compare timing of the two possible fitters for algo.farrington (here using S4)
system.time( sts1 <- farrington(sts, control=list(range=c(500:1000),
                       fitFun="algo.farrington.fitGLM.fast")))
system.time( sts2 <- farrington(sts, control=list(range=c(500:1000),
                       fitFun="algo.farrington.fitGLM")))

#Check if results are the same
sum(upperbound(sts1) - upperbound(sts2))
}
}
\author{M. \enc{Höhle}{Hoehle}}
\seealso{\code{\link{algo.farrington.fitGLM}},\code{\link{algo.farrington.threshold}}}
\keyword{classif}
\source{A statistical algorithm for the early detection of outbreaks of infectious disease, Farrington, C.P., Andrews, N.J, Beale A.D. and Catchpole, M.A. (1996), J. R. Statist. Soc. A, 159, 547-563.}
<<echo=F>>=

algo.farrington <- function(disProgObj, control=list(range=NULL, b=3, w=3, reweight=TRUE, verbose=FALSE,alpha=0.01,trend=TRUE,limit54=c(5,4),powertrans="2/3",fitFun=c("algo.farrington.fitGLM.fast","algo.farrington.fitGLM","algo.farrington.fitGLM.populationOffset"))) { 
  #Fetch observed
  observed <- disProgObj$observed
  freq <- disProgObj$freq
  epochStr <- switch( as.character(freq), "12" = "1 month","52" =  "1 week","365" = "1 day")
  #Fetch population (if it exists)
  if (!is.null(disProgObj$populationFrac)) {
    population <- disProgObj$populationFrac } 
  else { 
    population <- rep(1,length(observed))
  }

  ######################################################################
  # Fix missing control options
  ######################################################################
  if (is.null(control$range)) {
    control$range <- (freq*control$b - control$w):length(observed)
  }
  if (is.null(control$b))        {control$b=5}
  if (is.null(control$w))        {control$w=3}
  if (is.null(control$reweight)) {control$reweight=TRUE}
  if (is.null(control$verbose))  {control$verbose=FALSE}
  if (is.null(control$alpha))    {control$alpha=0.05}
  if (is.null(control$trend))    {control$trend=TRUE}
  if (is.null(control$plot))     {control$plot=FALSE}
  if (is.null(control$limit54))  {control$limit54=c(5,4)}
  if (is.null(control$powertrans)){control$powertrans="2/3"}
  if (is.null(control$fitFun))   {
    control$fitFun="algo.farrington.fitGLM.fast"
  } else {
    control$fitFun <- match.arg(control$fitFun, c("algo.farrington.fitGLM.fast","algo.farrington.fitGLM","algo.farrington.fitGLM.populationOffset"))
  }

  #Use special Date class mechanism to find reference months/weeks/days
  if (is.null(disProgObj[["epochAsDate",exact=TRUE]])) { 
    epochAsDate <- FALSE 
  } else { 
    epochAsDate <- disProgObj[["epochAsDate",exact=TRUE]] 
  }
    
  #check options
  if (!((control$limit54[1] >= 0) &  (control$limit54[2] > 0))) {
    stop("The limit54 arguments are out of bounds: cases >= 0 and period > 0.")
  }

  # initialize the necessary vectors
  alarm <- matrix(data = 0, nrow = length(control$range), ncol = 1)
  trend <- matrix(data = 0, nrow = length(control$range), ncol = 1)
  upperbound <- matrix(data = 0, nrow = length(control$range), ncol = 1)
  # predictive distribution
  pd <- matrix(data = 0, nrow = length(control$range), ncol = 2)

  # Define objects
  n <- control$b*(2*control$w+1)
  
  # 2: Fit of the initial model and first estimation of mean and dispersion
  #    parameter
  for (k in control$range) {
    # transform the observed vector in the way
    # that the timepoint to be evaluated is at last position
    #shortObserved <- observed[1:(maxRange - k + 1)]

    if (control$verbose) { cat("k=",k,"\n")}

    #Find index of all epochs, which are to be used as reference values
    #i.e. with index k-w,..,k+w 
    #in the years (current year)-1,...,(current year)-b
    if (!epochAsDate) {
      wtimeAll <- NULL
      for (i in control$b:1){
        wtimeAll <- append(wtimeAll,seq(k-freq*i-control$w,k-freq*i+control$w,by=1))
      }
      #Select them as reference values
      wtime <- wtimeAll[wtimeAll>0]
      if (length(wtimeAll) != length(wtime)) {
        warning("Some reference values did not exist (index<1).")
      }
    } else { #Alternative approach using Dates
      t0 <- as.Date(disProgObj$week[k], origin="1970-01-01")
      wtime <- refvalIdxByDate( t0=t0, b=control$b, w=control$w, epochStr=epochStr, epochs=disProgObj$week)
    }
    
    #Extract values from indices
    response <- observed[wtime]
    pop <- population[wtime]

    if (control$verbose) { print(response)}

    ######################################################################
    #Fit the model with overdispersion -- the initial fit
    ######################################################################
    #New feature: fitFun can now be the fast function for fitting the GLM
    model <- do.call(control$fitFun, args=list(response=response,wtime=wtime,population=pop,timeTrend=control$trend,reweight=control$reweight))

    #Stupid check to pass on NULL values from the algo.farrington.fitGLM proc.
    if (is.null(model)) return(model)

    ######################################################################
    #Time trend
    #
    #Check whether to include time trend, to do this we need to check whether
    #1) wtime is signifcant at the 95lvl
    #2) the predicted value is not larger than any observed value
    #3) the historical data span at least 3 years.
    doTrend <- control$trend
    if (control$trend) {
      #is the p-value for the trend significant (0.05) level
      p <- summary.glm(model)$coefficients["wtime",4]
      significant <- (p < 0.05)
      #prediction for time k
      mu0Hat <- predict.glm(model,data.frame(wtime=c(k),population=population[k]),type="response")
      #have to use at least three years of data to allow for a trend
      atLeastThreeYears <- (control$b>=3)
      #no horrible predictions
      noExtrapolation <- mu0Hat <= max(response)
     
      #All 3 criteria have to be met in order to include the trend. Otherwise
      #it is removed. Only necessary to check this if a trend is requested.
      if (!(atLeastThreeYears && significant && noExtrapolation)) {
        doTrend <- FALSE
        model <- do.call(control$fitFun, args=list(response=response,wtime=wtime,population=pop,timeTrend=FALSE,reweight=control$reweight))
      }
    } else {
      doTrend <- FALSE
    }
    #done with time trend
    ######################################################################
    
    ######################################################################
    # Calculate prediction & confidence interval                         #
    ######################################################################
    #Predict value - note that the se is the mean CI
    #and not the prediction error of a single observation
    pred <- predict.glm(model,data.frame(wtime=c(k),population=population[k]),dispersion=model$phi,
                        type="response",se.fit=TRUE)
    #Calculate lower and upper threshold
    lu <- algo.farrington.threshold(pred,model$phi,skewness.transform=control$powertrans,alpha=control$alpha, observed[k])

    ######################################################################
    # If requested show a plot of the fit.
    ######################################################################
    if (control$plot) {
      #Compute all predictions
      data <- data.frame(wtime=seq(min(wtime),k,length=1000))
      preds <- predict(model,data,type="response",dispersion=model$phi)

      #Show a plot of the model fit.
      plot(c(wtime, k), c(response,observed[k]),ylim=range(c(observed[data$wtime],lu)),,xlab="time",ylab="No. infected",main=paste("Prediction at time t=",k," with b=",control$b,",w=",control$w,sep=""),pch=c(rep(1,length(wtime)),16))
      #Add the prediction
      lines(data$wtime,preds,col=1,pch=2)

      #Add the thresholds to the plot
      lines(rep(k,2),lu[1:2],col=3,lty=2)
    }


    ######################################################################
    #Postprocessing steps
    ######################################################################

    #Compute exceedance score unless less than 5 reports during last 4 weeks.
    #Changed in version 0.9-7 - current week is included now
    enoughCases <- (sum(observed[(k-control$limit54[2]+1):k])>=control$limit54[1])

    #18 May 2006: Bug/unexpected feature found by Y. Le Strat. 
    #the okHistory variable meant to protect against zero count problems,
    #but instead it resulted in exceedance score == 0 for low counts. 
    #Now removed to be concordant with the Farrington 1996 paper.
    X <- ifelse(enoughCases,(observed[k] - pred$fit) / (lu[2] - pred$fit),0)

    #Do we have an alarm -- i.e. is observation beyond CI??
    #upperbound only relevant if we can have an alarm (enoughCases)
    trend[k-min(control$range)+1] <- doTrend
    alarm[k-min(control$range)+1] <- (X>1)
    upperbound[k-min(control$range)+1] <- ifelse(enoughCases,lu[2],0)
    #Compute bounds of the predictive
    pd[k-min(control$range)+1,] <- lu[c(3,4)]

  }#done looping over all time points

  #Add name and data name to control object.
  control$name <- paste("farrington(",control$w,",",0,",",control$b,")",sep="")
  control$data <- paste(deparse(substitute(disProgObj)))
  #Add information about predictive distribution
  control$pd   <- pd

  # return alarm and upperbound vectors 
  result <- list(alarm = alarm, upperbound = upperbound, trend=trend, 
                 disProgObj=disProgObj, control=control) 
  class(result) <- "survRes" 

  #Done
  return(result) 
}

@ 


